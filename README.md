# Exploring-Task-Vector-Adaptation-in-Heterogeneous-Architectures
Task vector arithmetic is a recent paradigm for specializing pretrained models on downstream tasks. Task vectors, defined as the difference between fine-tuned and pretrained weights, can be combined arithmetically for model specialization (Ilharco et al., 2022). Typically applicable only to identical architectures, this project investigates their effectiveness in architecturally different models sharing a common backbone, applying arithmetic only to common parameters. We evaluate task vector arithmetic in three scenarios: forgetting via negation, learning via addition, and learning via analogy, incorporating elements of modular deep learning (Pfeiffer et al.,2023) in one case.
