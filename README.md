# Exploring-Task-Vector-Adaptation-in-Heterogeneous-Architectures
Task vector arithmetic is a recent paradigm for specializing pretrained models on downstream tasks. Task vectors, defined as the difference between fine-tuned and pretrained weights, can be combined arithmetically for model specialization (Ilharco et al., 2022). Typically applicable only to identical architectures, this project investigates their effectiveness in architecturally different models sharing a common backbone, applying arithmetic only to common parameters. We evaluate task vector arithmetic in three scenarios: forgetting via negation, learning via addition, and learning via analogy, incorporating elements of modular deep learning (Pfeiffer et al.,2023) in one case.

There's no need to run "Task Vector Computation" because all the task 
vectors are already computed and stored in the folder "task vectors".


The "Main Experiments"  cannot be directly executed because there are
dependencies of data which are on my personal Drive. 
